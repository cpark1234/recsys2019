{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "impression_embed_size = 200 # 853,540개 \n",
    "poi_embed_size = 50      # 13,352개\n",
    "filter_embed_size = 10   # 208개\n",
    "platform_embed_size = 5       # 55개\n",
    "city_embed_size = 50          # 34,752개\n",
    "sort_embed_size = 7 #interaction 빼고 one-hot\n",
    "device_embed_size = 3 # mobile, desktop, tablet one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epoch = 10\n",
    "batch_size = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_hidden_3 = 128 # 3rd layer number of neurons\n",
    "\n",
    "input_size = poi_embed_size+filter_embed_size+sort_embed_size+impression_embed_size+platform_embed_size+city_embed_size+\\\n",
    "device_embed_size+filter_embed_size + impression_embed_size + 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def neural_net(XX_input):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(XX_input, n_hidden_1,activation=tf.nn.relu)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2,activation=tf.nn.relu)\n",
    "#     layer_2_dropout = tf.layers.dropout(layer2, training=True)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    layer_3 = tf.layers.dense(layer_2, n_hidden_3, activation=tf.nn.relu)\n",
    "    out_layer = tf.layers.dense(layer_3, 1, activation=tf.nn.sigmoid)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functions as f\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "meta_df = pd.read_csv('./item_metadata.csv')\n",
    "submit_df = pd.read_csv('./submission_popular.csv')\n",
    "\n",
    "popular_df = f.get_popularity(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecube_server2/.conda/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "poi_idx = np.load('./npy/poi_names.npy', allow_pickle=True)\n",
    "poi_idx = list(poi_idx)\n",
    "impressions_idx = np.load('./npy/impressions_index.npy', allow_pickle=True)\n",
    "impressions_idx = list(impressions_idx)\n",
    "city_idx = np.load('./npy/city_names.npy', allow_pickle=True)\n",
    "city_idx = list(city_idx)\n",
    "platform_idx = np.load('./npy/platform_names.npy', allow_pickle=True)\n",
    "platform_idx = list(platform_idx)\n",
    "filter_idx = np.load('./npy/filter_merged.npy', allow_pickle=True)\n",
    "filter_idx = list(filter_idx)\n",
    "action_idx = np.load('./npy/action_type_names.npy', allow_pickle=True)\n",
    "action_idx = list(action_idx)\n",
    "\n",
    "sort_order_idx = np.load('./npy/sorting_names.npy', allow_pickle=True)\n",
    "sort_order_idx = list(sort_order_idx)\n",
    "device_idx = np.load('./npy/device_names.npy', allow_pickle=True)\n",
    "device_idx = list(device_idx)\n",
    "\n",
    "poi_embedding = tf.get_variable(\"poi_embedding\", [len(poi_idx), poi_embed_size])\n",
    "impression_embedding = tf.get_variable(\"impression_embedding\", [len(impressions_idx), impression_embed_size])\n",
    "city_embedding = tf.get_variable(\"city_embedding\", [len(city_idx), city_embed_size])\n",
    "platform_embedding = tf.get_variable(\"platform_embedding\", [len(platform_idx), platform_embed_size])\n",
    "filter_embedding = tf.get_variable(\"filter_embedding\", [len(filter_idx), filter_embed_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df[(train_df[\"action_type\"] == \"clickout item\")]     #train_label : (1,586,586, 12)\n",
    "batch_indexes = np.random.permutation(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-db9946ea3c44>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels= Y, logits= neural_net(X))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_function(domain, key):\n",
    "    \n",
    "    embedded_vector = None\n",
    "    \n",
    "    if domain == 'poi':\n",
    "        if key in poi_idx:\n",
    "            idx = poi_idx.index(key)\n",
    "        else:\n",
    "            idx = 0\n",
    "        embedded_vector = tf.nn.embedding_lookup(poi_embedding, idx)\n",
    "        \n",
    "    elif domain == 'impression':\n",
    "        if key in impressions_idx:\n",
    "            idx = impressions_idx.index(key)\n",
    "        else:\n",
    "            idx = 0\n",
    "        embedded_vector = tf.nn.embedding_lookup(impression_embedding, idx)\n",
    "    \n",
    "    elif domain == 'city':\n",
    "        if key in city_idx:\n",
    "            idx = city_idx.index(key)\n",
    "        else:\n",
    "            idx = 0\n",
    "        embedded_vector = tf.nn.embedding_lookup(city_embedding, idx)\n",
    "\n",
    "    elif domain == 'platform':\n",
    "        if key in platform_idx:\n",
    "            idx = platform_idx.index(key)\n",
    "        else:\n",
    "            idx = 0\n",
    "        embedded_vector = tf.nn.embedding_lookup(platform_embedding, idx)\n",
    "        \n",
    "    elif domain =='filter':\n",
    "        if key in filter_idx:\n",
    "            idx = filter_idx.index(key)\n",
    "        else:\n",
    "            idx = 0\n",
    "        embedded_vector = tf.nn.embedding_lookup(filter_embedding, idx)\n",
    "    elif domain =='sorting':\n",
    "        embedded_vector = np.zeros([sort_embed_size])\n",
    "        embedded_vector[sort_order_idx.index(key)] = 1\n",
    "        embedded_vector = tf.convert_to_tensor(embedded_vector, dtype=tf.float32)\n",
    "    elif domain =='device':\n",
    "        embedded_vector = np.zeros([device_embed_size])\n",
    "        embedded_vector[device_idx.index(key)] = 1\n",
    "        embedded_vector = tf.convert_to_tensor(embedded_vector, dtype=tf.float32)\n",
    "        \n",
    "    return embedded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # weighted sum. 자주 등장한 keyword의 embedding을 더 따르도록\n",
    "def getWeightedAverage(domain, domain_list, embed_size): \n",
    "    final_embed = np.zeros([embed_size])\n",
    "    count_dic = Counter(domain_list)\n",
    "    count_sum=0\n",
    "    for key in count_dic:\n",
    "        embed = embedding_function(domain, key)\n",
    "        final_embed += count_dic[key] * embed  \n",
    "        count_sum += count_dic[key]\n",
    "    \n",
    "    final_embed = final_embed/count_sum\n",
    "    \n",
    "    return final_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainData(train_x_df, train_label_batch):\n",
    "    \n",
    "    # poi \n",
    "    poi_list = train_x_df[(train_x_df['action_type']=='search for poi')]['reference']\n",
    "    if len(poi_list)==0:\n",
    "        final_embed_poi = tf.zeros([poi_embed_size])\n",
    "    else:\n",
    "        final_embed_poi = getWeightedAverage('poi', poi_list , poi_embed_size)\n",
    "    \n",
    "    \n",
    "    #filters (current filters 총합)\n",
    "    filter_list = train_x_df['current_filters'].dropna()\n",
    "    if len(filter_list)==0:\n",
    "        final_embed_filters = tf.zeros([filter_embed_size])\n",
    "    else:\n",
    "        filters = []\n",
    "        for item in filter_list:\n",
    "            item_split = f.string_to_array(item)\n",
    "            filters = np.concatenate((filters, item_split))\n",
    "        final_embed_filters = getWeightedAverage('filter', filters , filter_embed_size)\n",
    "    \n",
    "    #sort order\n",
    "    sort_orders = train_x_df[(train_x_df['action_type']=='change of sort order') & (train_x_df['reference']!='interaction sort button')][\"reference\"]\n",
    "    if len(sort_orders)==0:\n",
    "        final_embed_sorting = tf.zeros([sort_embed_size])\n",
    "    else:\n",
    "        sort_list=[]\n",
    "        for item in sort_orders:\n",
    "            sort_list.append(item)\n",
    "        final_embed_sorting = getWeightedAverage('sorting', sort_list , sort_embed_size)\n",
    "    \n",
    "    # 관심있는 item\n",
    "            #  'clickout item'\n",
    "            # 'interaction item deals',\n",
    "            #  'interaction item image',\n",
    "            #  'interaction item info',\n",
    "            #  'interaction item rating',\n",
    "            # 'search for item',\n",
    "    item_list = train_x_df[(train_x_df['action_type']=='clickout item')|(train_x_df['action_type']=='interaction item deals')\n",
    "                      |(train_x_df['action_type']=='interaction item image')|(train_x_df['action_type']=='interaction item info')\n",
    "                      |(train_x_df['action_type']=='interaction item rating')|(train_x_df['action_type']=='search for item')]['reference']\n",
    "    \n",
    "    if len(item_list)==0:\n",
    "        final_embed_items = tf.zeros([impression_embed_size])\n",
    "    else:\n",
    "        item_merged_list=[]\n",
    "        for item in item_list:\n",
    "            item_merged_list.append(item)\n",
    "        final_embed_items = getWeightedAverage('impression', item_merged_list , impression_embed_size)\n",
    "    \n",
    "    #platform\n",
    "    platform_embed = embedding_function('platform', train_label_batch['platform'])\n",
    "    \n",
    "    #city, search for destination 해서 검색해봤던 city들\n",
    "    city_embed = embedding_function('city', train_label_batch['city'])\n",
    "    \n",
    "    #device\n",
    "    device_embed = embedding_function('device', train_label_batch['device'])\n",
    "    \n",
    "    #current_filter\n",
    "    \n",
    "    filter_b = train_label_batch['current_filters']\n",
    "    if pd.isna(filter_b):\n",
    "        final_embed_current_filter = tf.zeros(filter_embed_size)\n",
    "    else:\n",
    "        filters_b = f.string_to_array(filter_b)\n",
    "        final_embed_current_filter = getWeightedAverage('filter', filters_b , filter_embed_size)\n",
    "    \n",
    "#     print('poi : ', np.shape(final_embed_poi))\n",
    "#     print('filters : ', np.shape(final_embed_filters))\n",
    "#     print('sorting : ', np.shape(final_embed_sorting))\n",
    "#     print('items : ', np.shape(final_embed_items))\n",
    "#     print('platform : ', np.shape(platform_embed))\n",
    "#     print('city : ', np.shape(city_embed))\n",
    "#     print('device : ', np.shape(device_embed))\n",
    "#     print('current filter : ', np.shape(final_embed_current_filter))\n",
    "    \n",
    "    \n",
    "    return tf.concat((final_embed_poi, final_embed_filters, final_embed_sorting, final_embed_items, platform_embed, city_embed, device_embed, final_embed_current_filter), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi :  (50,)\n",
      "filters :  (10,)\n",
      "sorting :  (7,)\n",
      "items :  (200,)\n",
      "platform :  (5,)\n",
      "city :  (50,)\n",
      "device :  (3,)\n",
      "current filter :  (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecube_server2/.conda/envs/recsys/lib/python3.6/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"concat_3:0\", shape=(537,), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 537), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-02c4c4334b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpopular_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\", h:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                             \u001b[0;34m'For reference, the tensor object was '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' which was passed to the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                             'feed with key ' + str(feed) + '.')\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m           \u001b[0msubfeed_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"concat_3:0\", shape=(537,), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 537), dtype=float32)."
     ]
    }
   ],
   "source": [
    "label_df = None\n",
    "for k in range(3):\n",
    "    idx = batch_indexes[k*batch_size:(k+1)*batch_size]\n",
    "    label_df = train_label.iloc[idx]\n",
    "    current_user = label_df[\"user_id\"]\n",
    "    for i in range(3):  # batch size\n",
    "        train_df_current_user = train_df[train_df[\"user_id\"]==current_user.iloc[i]]\n",
    "        train_idx = train_df_current_user.index.get_loc(label_df.iloc[i].name)\n",
    "        train_df_trucated = train_df_current_user[:train_idx]\n",
    "        \n",
    "        if len(train_df_trucated)>30:\n",
    "            train_df_trucated = train_df_trucated[-30:]\n",
    "        \n",
    "        train_label_batch = label_df.iloc[i][\"reference\"]  # 클릭한 reference number\n",
    "        train_candidates = label_df.iloc[i]['impressions']\n",
    "        train_candidates = f.string_to_array(train_candidates)   \n",
    "        \n",
    "        candidate_prices = label_df.iloc[i]['prices']\n",
    "        candidate_prices = f.string_to_array(candidate_prices)\n",
    "        candidate_prices = np.array(candidate_prices).astype(int)\n",
    "        # train_candidates 중에 정답을 고르게 된다. \n",
    "        # 학습 할 때는 선택 안한 애는 라벨 0 , 선택한 애는 라벨 1. infer할 때는 0.9 인 애를 고르면 됨.\n",
    "        context_vector = getTrainData(train_df_trucated, label_df.iloc[i])  # 현재 335 크기 context vector\n",
    "        \n",
    "        real_labels = np.zeros([len(train_candidates)])\n",
    "        real_labels[train_candidates.index(train_label_batch)] = 1\n",
    "        \n",
    "        for h in range(len(train_candidates)):\n",
    "            candi = train_candidates[h]\n",
    "            price_candi = candidate_prices[h]\n",
    "            candidate_vector = embedding_function('impression', candi)\n",
    "            \n",
    "            pop_temp = popular_df[popular_df['reference']==candi]\n",
    "            if len(pop_temp)==0:\n",
    "                popularity= 0\n",
    "            else: \n",
    "                popularity = int(popular_df[popular_df['reference']==candi]['n_clicks'])\n",
    "                if popularity > 10:\n",
    "                    popularity = 10\n",
    "            popular_vector = (popularity - 0)/(10-0)\n",
    "            \n",
    "            if price_candi < 15:\n",
    "                price_candi = 15\n",
    "            elif price_candi > 325:\n",
    "                price_candi = 325\n",
    "            price_vector = (price_candi - 15)/(325-15)\n",
    "            XX = tf.concat((context_vector, candidate_vector, tf.convert_to_tensor(np.array([popular_vector, price_vector]),dtype=tf.float32)),axis=0)\n",
    "            YY = real_labels[h]\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={X: XX, Y: YY})\n",
    "            print(\"i: \", i ,\", h:\", h, \"loss:\", loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = np.array([],dtype=int)\n",
    "\n",
    "# for i in range(len(impression_total)):\n",
    "#     impressions = impression_total.iloc[i][\"impressions\"]\n",
    "#     s = f.string_to_array(impressions)\n",
    "#     xx = np.array(s)\n",
    "#     xx = xx.astype(int)\n",
    "#     merged = np.concatenate((merged, xx))\n",
    "#     if i % 10000 ==0:\n",
    "#         print(\"iter : \", i, \"len:\" ,len(merged))\n",
    "#         merged = np.unique(merged)\n",
    "#         print(\"merged : \", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
