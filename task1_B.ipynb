{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle \n",
    "path = Path('./')\n",
    "train_path = path.joinpath('train.csv')\n",
    "test_path = path.joinpath('test.csv')\n",
    "submission_path = path.joinpath('submission_popular.csv')\n",
    "metadata_path = path.joinpath('item_metadata.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_meta = pd.read_csv(metadata_path)\n",
    "df_submission = pd.read_csv(submission_path)\n",
    "\n",
    "action_list = set(df_train['action_type'].values)\n",
    "reference_list = set(df_train['reference'].values)\n",
    "platform_list = set(df_train['platform'].values)\n",
    "city_list = set(df_train['city'].values)\n",
    "device_list = set(df_train['device'].values)\n",
    "filter_list = set(df_train['current_filters'].values)\n",
    "filter_list =  [x for x in filter_list if str(x) != 'nan']\n",
    "\n",
    "action_list = list(action_list)\n",
    "action_list.sort()\n",
    "\n",
    "reference_list = list(reference_list)\n",
    "reference_list.sort()\n",
    "\n",
    "platform_list = list(platform_list)\n",
    "platform_list.sort()\n",
    "\n",
    "city_list = list(city_list)\n",
    "city_list.sort()\n",
    "\n",
    "device_list = list(device_list)\n",
    "device_list.sort()\n",
    "\n",
    "filter_list = list(filter_list)\n",
    "filter_list.sort()\n",
    "\n",
    "#테스트 데이터에서 마지막 reference가 nan인지 확인\n",
    "def check_data(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        if(isinstance(data[i][-1,5], float)):\n",
    "            if(math.isnan(data[i][-1,5]) == True):\n",
    "                pass\n",
    "            else:\n",
    "                print('error')\n",
    "                print(data[i])\n",
    "                max_error +=1\n",
    "        else:\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "#테스트 데이터에서 reference가 nan일때 그 전 action이 click out인지 확인\n",
    "def check_data_alpha(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float)):\n",
    "                if(math.isnan(data[i][j,5]) == True):\n",
    "                     if(data[i][j,4] != 'clickout item'):\n",
    "                        print(data[i])\n",
    "                        max_error +=1\n",
    "        \n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "            \n",
    "#테스트 데이터에서 reference에 하나라도 nan이 있는지 판별 있다면 그 중 적어도 하나는 action이 click out인지 판별\n",
    "def check_data_beta(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        error_data_number = 0\n",
    "        click_check = False\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                error_data_number += 1\n",
    "        if(error_data_number == 0):\n",
    "            print(data[i])\n",
    "            print('error')\n",
    "            max_error +=1\n",
    "        if(error_data_number != 0):\n",
    "            for j in range(len(data[i])):\n",
    "                if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                    if(data[i][j,4] == 'clickout item'):\n",
    "                        click_check = True\n",
    "        if(click_check == False):\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "            #print(data[i])\n",
    "            #print('datas')\n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "            \n",
    "def check_inference(data):\n",
    "    datalength = len(data)\n",
    "    maxlength = 0\n",
    "    correct_data = 0\n",
    "    new_data = []\n",
    "    for i in range(datalength):\n",
    "        if(data[i][-1,5] in data[i][-1,10].split('|')):\n",
    "            new_data.append(data[i])\n",
    "            correct_data += 1\n",
    "        else:\n",
    "            pass\n",
    "            #print(i)\n",
    "            #print('error')\n",
    "            #maxlength +=1\n",
    "    print(correct_data)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 트레이닝 데이터를 세션 단위로 나눔\n",
    "def make_data(data): \n",
    "    data_split = []\n",
    "    temp_data = []\n",
    "    train_size = len(data)\n",
    "    action_index = 1\n",
    "    for i in range(train_size):\n",
    "        if(i==0):\n",
    "            temp_data.append(data[i])\n",
    "        if(i>0):\n",
    "            if(data[i,action_index] == data[i-1,action_index]):\n",
    "                temp_data.append(data[i])\n",
    "            else:\n",
    "                data_split.append(np.array(temp_data))\n",
    "                temp_data = []\n",
    "                temp_data.append(data[i])\n",
    "    return data_split\n",
    "\n",
    "train_data_split = make_data(df_train.values) #session 별로 데이터 나눔\n",
    "\n",
    "#트레이닝 데이터에서 click out이 존재하는 데이터만 뽑아냄\n",
    "#트레이닝 데이터에서 끝부분이 click out이 되도록 뒷부분을 자름\n",
    "def data_preprocess(data): \n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        check_data = False\n",
    "        final_click = 0\n",
    "        for j in range(len(data[i])):\n",
    "            if(data[i][j,4] =='clickout item'):\n",
    "                final_click = j\n",
    "                check_data = True\n",
    "        if(final_click < len(data[i])-1 and check_data):\n",
    "            new_data.append(np.delete(data[i],np.s_[final_click+1:],0))\n",
    "        elif(check_data):\n",
    "            new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data_preprocess = data_preprocess(train_data_split) # session 끝에 무조건 clickout이 존재\n",
    "train_data_preprocess = check_inference(train_data_preprocess) # impresssion 안에 무조건 답이 존재 하는것!!\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "#reference embedding 보조 함수\n",
    "items = df_meta['item_id'].values\n",
    "properties = df_meta['properties'].values\n",
    "all_items_test = []\n",
    "for i in range(len(properties)):\n",
    "    p_data = properties[i].split('|')\n",
    "    all_items_test += p_data\n",
    "set_items = set(all_items_test)\n",
    "\n",
    "list_items = list(set_items)\n",
    "list_items.sort()\n",
    "def embedding_item_diction():\n",
    "    all_items = {}\n",
    "    for i in range(len(properties)):\n",
    "        all_items[items[i]] = properties[i].split('|')\n",
    "    embedding_items = {}\n",
    "    #print(len(list_items))\n",
    "    for keys in all_items.keys():\n",
    "        item_embeddings = [0]*len(list_items)\n",
    "        for i in range(len(list_items)):\n",
    "            if list_items[i] in all_items[keys]:\n",
    "                item_embeddings[i] = 1\n",
    "        embedding_items[keys] = item_embeddings\n",
    "    return embedding_items\n",
    "reference_diction = embedding_item_diction()\n",
    "\n",
    "#filter embedding 보조함수\n",
    "all_filters = []\n",
    "for i in range(len(filter_list)):\n",
    "    if i>0:\n",
    "        all_filters += filter_list[i].split('|')\n",
    "all_filter_set = set(all_filters)\n",
    "all_filter_list = list(all_filter_set)\n",
    "all_filter_list.sort()\n",
    "def filter_embedding(data, version):\n",
    "    embedding_data = [0]*202\n",
    "    if version == 1:\n",
    "        datalist = data.split('|')\n",
    "        for filters in datalist:\n",
    "            embedding_data[all_filter_list.index(filters)] = 1\n",
    "        return embedding_data\n",
    "    \n",
    "def one_hot_encoding(number, length):\n",
    "    return [int(i==number) for i in range(length)]\n",
    "\n",
    "def sum_list(list1, list2):\n",
    "    length = len(list1)\n",
    "    final_list = []\n",
    "    for i in range(length):\n",
    "        final_list.append(list1[i] +list2[i])\n",
    "    return final_list\n",
    "\n",
    "# 전체 action embedding\n",
    "def embedding(data, action_index):\n",
    "    if action_index == 0: #user_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 1: # session_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 2: # timestep dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 3: #step dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 4: #action_type dim 10\n",
    "        #print(one_hot_encoding(action_list.index(data), 10))\n",
    "        return one_hot_encoding(action_list.index(data), 10)\n",
    "    \n",
    "    if action_index == 5: #reference dim 157\n",
    "        if RepresentsInt(data):\n",
    "            return reference_diction[int(data)]\n",
    "        else:\n",
    "            return [0]*157\n",
    "        \n",
    "    if action_index == 6: #platform dim 55\n",
    "        return one_hot_encoding(platform_list.index(data), 55)\n",
    "    \n",
    "    if action_index == 7: #city dim 0\n",
    "        #return city_list.index(data)\n",
    "        return []\n",
    "    \n",
    "    if action_index == 8: #device dim 3\n",
    "        return one_hot_encoding(device_list.index(data), 3)\n",
    "    \n",
    "    if action_index == 9: #current_filters dim 202\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*202\n",
    "        else:\n",
    "            return filter_embedding(data, 1)\n",
    "        \n",
    "    if action_index == 10: #impressions dim 157\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*157\n",
    "        else:\n",
    "            impression_embedding = [0]*157\n",
    "            impression_list = data.split('|')\n",
    "            for impression in impression_list:\n",
    "                try:\n",
    "                    impression_embedding =sum_list(impression_embedding, reference_diction[int(impression)])\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "                    #print(int(impression))\n",
    "            '''\n",
    "            if(len(impression_embedding) < 3925):\n",
    "                impression_embedding += [0]*(3925-len(impression_embedding))   \n",
    "            '''\n",
    "            return impression_embedding\n",
    "            \n",
    "        \n",
    "    if action_index == 11: #prices dim 25\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*25\n",
    "        else:\n",
    "            price_embedding = []\n",
    "            for prices in data.split('|'):\n",
    "                price_embedding.append(int(prices))\n",
    "            if(len(price_embedding) < 25):\n",
    "                price_embedding += [0]*(25-len(price_embedding))   \n",
    "            return price_embedding\n",
    "        \n",
    "def check_last_click(data):\n",
    "    data_number = 0\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][-1,4] == 'clickout item'):\n",
    "            data_number +=1\n",
    "    return data_number\n",
    "\n",
    "action_type_length = 10\n",
    "reference_length = 157\n",
    "platform_length = 55\n",
    "device_length = 3\n",
    "current_filters_length = 202\n",
    "impresssions_length = 3925\n",
    "prices_length = 25\n",
    "def addVector(vector1, vector2, coef, action_num):\n",
    "    new_action_type = []\n",
    "    new_reference = []\n",
    "    new_platform = []\n",
    "    new_device = []\n",
    "    new_filters = []\n",
    "    new_impressions = []\n",
    "    new_prices = [] \n",
    "    #print(len(vector1))\n",
    "    #print(len(vector2))\n",
    "    for i in range(10):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "    \n",
    "    for i in range(10, 167):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    for i in range(167, 222):\n",
    "        new_action_type.append(vector1[i])\n",
    "    \n",
    "    for i in range(222, 225):\n",
    "        new_action_type.append(vector1[i])\n",
    "        \n",
    "    for i in range(225, 427):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    return new_action_type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeEmbeddingVector(data):\n",
    "    item_vector = [] # item data embedding\n",
    "    num_action = len(data)\n",
    "    #correct_index = []\n",
    "    session_vector = []\n",
    "    for i in range(num_action): #거꾸로\n",
    "        temp_vector = []\n",
    "        for j in range(10):\n",
    "            temp_vector += embedding(data[i,j], j) #10+157+55+3+202 = 427\n",
    "        if(i == num_action-1):\n",
    "            temp_vector[10:167] = [0]*157 # 마지막 action의 reference를 지움\n",
    "        session_vector.append(temp_vector)\n",
    "    # 10+314+202+55+3 = 584\n",
    "    '''\n",
    "    for j in range(12):\n",
    "        timestep_vector += embedding(data[i,j], j) \n",
    "\n",
    "    if(i == num_action-1):\n",
    "        for idx in range(10, 167):\n",
    "            timestep_vector[idx] = 0 # reference 부분을 0으로 만듦\n",
    "    session_vector = addVector(session_vector, timestep_vector, time_delay_coef, num_action-1-i) #session vector 생성\n",
    "    '''\n",
    "    #item_vector.append(reference_diction[int(data[-1,5])])\n",
    "    temp_impressions = data[-1, 10].split('|')\n",
    "    for i in range(len(temp_impressions)):\n",
    "        if(data[-1,5] == temp_impressions[i]):\n",
    "            correct_index = i\n",
    "        item_vector.append(reference_diction[int(temp_impressions[i])])\n",
    "    return session_vector, item_vector, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevec, itemvec, crindex = MakeEmbeddingVector(train_data_preprocess[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sevec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemvec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sevec[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.743267391538431\n"
     ]
    }
   ],
   "source": [
    "datalength = len(train_data_preprocess)\n",
    "average_step =0\n",
    "steps = []\n",
    "for i in range(datalength):\n",
    "    average_step += train_data_preprocess[i][-1,3]\n",
    "    steps.append(train_data_preprocess[i][-1,3])\n",
    "print(average_step/datalength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 35, 7, 55, 3, 1, 1, 7, 1, 34]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMedian(a):\n",
    "    a_len = len(a)                # 배열 요소들의 전체 개수 구하기\n",
    "    if (a_len == 0): return None  # 빈 배열은 에러 반환\n",
    "    a_center = int(a_len / 2)     # 요소 개수의 절반값 구하기\n",
    "\n",
    "    if (a_len % 2 == 1):   # 요소 개수가 홀수면\n",
    "        return a[a_center]   # 홀수 개수인 배열에서는 중간 요소를 그대로 반환\n",
    "    else:\n",
    "        return (a[a_center - 1] + a[a_center]) / 2.0 # 짝수 개 요소는, 중간 두 수의 평균 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMedian(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(session, length = 10):\n",
    "    if(len(session) < length):\n",
    "        for i in range(length-len(session)):\n",
    "            session.append([0]*427)\n",
    "    else:\n",
    "        session = session[len(session)-length:len(session)]\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sevec = padding(sevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sevec[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hidden_size = 157\n",
    "sequence_length = 10\n",
    "embedding_length = 427\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session_placeholder = tf.placeholder(tf.float32, shape=(sequence_length, embedding_length))\n",
    "item_placeholder = tf.placeholder(tf.float32, shape=(None,hidden_size))\n",
    "item_index = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnNetwork():\n",
    "    rnn_input = tf.expand_dims(session_placeholder,0)\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    output, state = tf.nn.dynamic_rnn(cell, rnn_input, dtype = tf.float32)\n",
    "    return output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef denseNet(rnn_output):\\n    final_output = []\\n    y = tf.map_fn(lambda x: x, item_placeholder)\\n    for rnn_output in y:\\n        dense_input = tf.concat([rnn_output,tf.expand_dims(item_placeholder[0], 0)],1)\\n        dense_layer1 = tf.layers.dense(dense_input, 200, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\\n        dense_layer2 = tf.layers.dense(dense_layer1, 50, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\\n        dense_layer3 = tf.layers.dense(dense_layer2, 1, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\\n        final_output.append(dense_layer3)\\n    return final_output\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def denseNet(rnn_output):\n",
    "    final_output = []\n",
    "    y = tf.map_fn(lambda x: x, item_placeholder)\n",
    "    for rnn_output in y:\n",
    "        dense_input = tf.concat([rnn_output,tf.expand_dims(item_placeholder[0], 0)],1)\n",
    "        dense_layer1 = tf.layers.dense(dense_input, 200, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dense_layer2 = tf.layers.dense(dense_layer1, 50, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dense_layer3 = tf.layers.dense(dense_layer2, 1, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        final_output.append(dense_layer3)\n",
    "    return final_output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denseNet(rnn_output):\n",
    "    concat_input = tf.map_fn(lambda x: tf.concat([rnn_output,tf.expand_dims(x,0)], 1), item_placeholder)\n",
    "    dense_layer1 = tf.map_fn(lambda x: tf.layers.dense(x, 200, activation=tf.nn.sigmoid), concat_input)\n",
    "    dense_layer2 = tf.map_fn(lambda x: tf.layers.dense(x, 50, activation=tf.nn.sigmoid), dense_layer1)\n",
    "    dense_layer3 = tf.map_fn(lambda x: tf.layers.dense(x, 1, activation=tf.nn.sigmoid), dense_layer2)\n",
    "    return dense_layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef denseNet(rnn_output):\\n    final_output = []\\n    concat_input = tf.map_fn(lambda x: tf.concat([rnn_output,x], 1), item_placeholder)\\n    dense_layer1 = tf.map_fn(lambda x: tf.layers.dense(x, 200, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), concat_input)\\n    dense_layer2 = tf.map_fn(lambda x: tf.layers.dense(x, 50, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), dense_layer1)\\n    dense_layer3 = tf.map_fn(lambda x: tf.layers.dense(x, 1, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), dense_layer2)\\n    return dense_layer3\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def denseNet(rnn_output):\n",
    "    final_output = []\n",
    "    concat_input = tf.map_fn(lambda x: tf.concat([rnn_output,x], 1), item_placeholder)\n",
    "    dense_layer1 = tf.map_fn(lambda x: tf.layers.dense(x, 200, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), concat_input)\n",
    "    dense_layer2 = tf.map_fn(lambda x: tf.layers.dense(x, 50, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), dense_layer1)\n",
    "    dense_layer3 = tf.map_fn(lambda x: tf.layers.dense(x, 1, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), dense_layer2)\n",
    "    return dense_layer3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-95ddaabd2643>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-20-95ddaabd2643>:4: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "rnn_output = rnnNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'map/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 1, 314) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.map_fn(lambda x: tf.concat([rnn_output,tf.expand_dims(x,0)], 1), item_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-599f94f9b3d2>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "scores = tf.squeeze(denseNet(rnn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(tf.math.log(tf.sigmoid(scores[item_index] - scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Log_1:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(tf.sigmoid(scores[item_index] - scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Neg:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = tf.reduce_mean(tf.sigmoid(scores- scores[item_index]) + tf.sigmoid(tf.math.pow(scores,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MRS(score, idx):\n",
    "    return (list(np.squeeze(np.argsort(score, axis = 0))).index(idx)+1)/np.shape(score)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevec, itemvec, crindex = MakeEmbeddingVector(train_data_preprocess[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_items, correct_index = MakeEmbeddingVector(train_data_preprocess[0])\n",
    "batch_data = padding(batch_data)\n",
    "batch_data  = np.asarray(batch_data)         \n",
    "batch_items = np.asarray(batch_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 427)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 157)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "output index is :  22\n",
      "correct index is :  0\n",
      "corrent rs is :  0.16\n",
      "mean loss is :  1.009710431098938\n",
      "mean reciprocal score is :  0.5599999999999999\n",
      "data num is :  2\n",
      "step 501\n",
      "output index is :  7\n",
      "correct index is :  6\n",
      "corrent rs is :  0.8181818181818182\n",
      "mean loss is :  1.0075092267990113\n",
      "mean reciprocal score is :  0.4957373111145002\n",
      "data num is :  500\n",
      "step 1001\n",
      "output index is :  15\n",
      "correct index is :  1\n",
      "corrent rs is :  0.12\n",
      "mean loss is :  1.0046035382747651\n",
      "mean reciprocal score is :  0.4778005840921017\n",
      "data num is :  500\n",
      "step 1501\n",
      "output index is :  22\n",
      "correct index is :  1\n",
      "corrent rs is :  0.84\n",
      "mean loss is :  1.003242180109024\n",
      "mean reciprocal score is :  0.48422281389352984\n",
      "data num is :  500\n",
      "step 2001\n",
      "output index is :  5\n",
      "correct index is :  1\n",
      "corrent rs is :  0.8333333333333334\n",
      "mean loss is :  1.0025081362724304\n",
      "mean reciprocal score is :  0.48005438741998685\n",
      "data num is :  500\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "all_data_number = len(train_data_preprocess)\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    mean_loss = 0\n",
    "    mean_rs = 0\n",
    "    data_number = 0\n",
    "    for i in range(all_data_number):\n",
    "        #print(i)\n",
    "        #if i>100:\n",
    "        #    break\n",
    "        \n",
    "        try:\n",
    "            batch_data, batch_items, correct_index = MakeEmbeddingVector(train_data_preprocess[i])\n",
    "            batch_data = padding(batch_data)\n",
    "            batch_data  = np.asarray(batch_data)         \n",
    "            batch_items = np.asarray(batch_items)\n",
    "\n",
    "            sess.run(optimizer, feed_dict = {session_placeholder : batch_data, item_placeholder : batch_items, item_index : correct_index})\n",
    "            ls = sess.run(loss2, feed_dict = {session_placeholder : batch_data, item_placeholder : batch_items, item_index : correct_index})\n",
    "            _score = sess.run(scores, feed_dict = {session_placeholder : batch_data, item_placeholder : batch_items, item_index : correct_index})\n",
    "            #print(ls)\n",
    "            #print(_score)\n",
    "            #print(correct_index)\n",
    "            current_rs = get_MRS(_score, correct_index)\n",
    "\n",
    "            #print(np.argmax(_score, axis = 0)[0])\n",
    "            #print(correct_index[0])\n",
    "\n",
    "            mean_rs += current_rs \n",
    "            mean_loss += ls\n",
    "            data_number += 1\n",
    "            if (step % 500 == 1):\n",
    "                #data_index = sess.run(tf.argmax(scores)[0], feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "\n",
    "                #acc = sess.run(accuracy, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "                print('step', step )\n",
    "                print(\"output index is : \", np.argmax(_score, axis = 0))\n",
    "                print(\"correct index is : \", correct_index)\n",
    "                print(\"corrent rs is : \", current_rs)\n",
    "                print(\"mean loss is : \", mean_loss / data_number)\n",
    "                print(\"mean reciprocal score is : \", mean_rs / data_number)\n",
    "                print(\"data num is : \", data_number)\n",
    "                mean_rs =0\n",
    "                mean_loss = 0\n",
    "                data_number = 0\n",
    "            step +=1\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "all_data_number = len(session_data)\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    mean_loss = 0\n",
    "    mean_rs = 0\n",
    "    data_number = 0\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    epoch = 20\n",
    "    for i in range(all_data_number*epoch//batch_size):\n",
    "        print(i)\n",
    "        if i>100:\n",
    "            break\n",
    "        \n",
    "        #$try:\n",
    "        #시작지점과 끝지점 지정\n",
    "        if(end > all_data_number):\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "\n",
    "        #batch 데이터 생성\n",
    "        batch_data = []\n",
    "        batch_items = []\n",
    "        correct_index =  []\n",
    "        for j in range(start,end):\n",
    "            temp_data, temp_items, temp_index = MakeEmbeddingVector(train_data_preprocess[j])\n",
    "            batch_data.append(temp_data)\n",
    "            batch_items.append(temp_items)\n",
    "            correct_index.append(temp_index)\n",
    "\n",
    "       # batch_data  = np.asarray(batch_data)\n",
    "       # batch_items = np.asarray(batch_items)\n",
    "       # batch_items = np.transpose(batch_items,(1,0,2))\n",
    "       # temp_index  = np.asarray(temp_index)\n",
    "\n",
    "        sess.run(optimizer, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "\n",
    "        #print(np.argmax(_score, axis = 0)[0])\n",
    "        #print(correct_index[0])\n",
    "        data_number +=1\n",
    "        if (step % 100 == 1):\n",
    "            ls = sess.run(loss2, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "            _score = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "            \n",
    "            current_rs = 0\n",
    "            for i in range(batch_size):\n",
    "                current_rs += get_MRS(_score[:,i,:], correct_index[0])\n",
    "            current_rs /= 32\n",
    "            print('step', step )\n",
    "            #print(\"output index is : \", np.argmax(_score, axis = 0)[0])\n",
    "            #print(\"correct index is : \", correct_index[0])\n",
    "            print(\"corrent rs is : \", current_rs)\n",
    "            print(\"mean loss is : \", ls)\n",
    "            print(\"mean reciprocal score is : \", current_rs)\n",
    "            #print(\"data num is : \", data_number)\n",
    "\n",
    "            #data_number = 0\n",
    "        step += 1\n",
    "        start += batch_size\n",
    "        end += batch_size\n",
    "                \n",
    "       # except:\n",
    "       #     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
