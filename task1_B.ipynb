{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle \n",
    "path = Path('./')\n",
    "train_path = path.joinpath('train.csv')\n",
    "test_path = path.joinpath('test.csv')\n",
    "submission_path = path.joinpath('submission_popular.csv')\n",
    "metadata_path = path.joinpath('item_metadata.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_meta = pd.read_csv(metadata_path)\n",
    "df_submission = pd.read_csv(submission_path)\n",
    "\n",
    "action_list = set(df_train['action_type'].values)\n",
    "reference_list = set(df_train['reference'].values)\n",
    "platform_list = set(df_train['platform'].values)\n",
    "city_list = set(df_train['city'].values)\n",
    "device_list = set(df_train['device'].values)\n",
    "filter_list = set(df_train['current_filters'].values)\n",
    "filter_list =  [x for x in filter_list if str(x) != 'nan']\n",
    "\n",
    "action_list = list(action_list)\n",
    "action_list.sort()\n",
    "\n",
    "reference_list = list(reference_list)\n",
    "reference_list.sort()\n",
    "\n",
    "platform_list = list(platform_list)\n",
    "platform_list.sort()\n",
    "\n",
    "city_list = list(city_list)\n",
    "city_list.sort()\n",
    "\n",
    "device_list = list(device_list)\n",
    "device_list.sort()\n",
    "\n",
    "filter_list = list(filter_list)\n",
    "filter_list.sort()\n",
    "\n",
    "#테스트 데이터에서 마지막 reference가 nan인지 확인\n",
    "def check_data(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        if(isinstance(data[i][-1,5], float)):\n",
    "            if(math.isnan(data[i][-1,5]) == True):\n",
    "                pass\n",
    "            else:\n",
    "                print('error')\n",
    "                print(data[i])\n",
    "                max_error +=1\n",
    "        else:\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "#테스트 데이터에서 reference가 nan일때 그 전 action이 click out인지 확인\n",
    "def check_data_alpha(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float)):\n",
    "                if(math.isnan(data[i][j,5]) == True):\n",
    "                     if(data[i][j,4] != 'clickout item'):\n",
    "                        print(data[i])\n",
    "                        max_error +=1\n",
    "        \n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "            \n",
    "#테스트 데이터에서 reference에 하나라도 nan이 있는지 판별 있다면 그 중 적어도 하나는 action이 click out인지 판별\n",
    "def check_data_beta(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        error_data_number = 0\n",
    "        click_check = False\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                error_data_number += 1\n",
    "        if(error_data_number == 0):\n",
    "            print(data[i])\n",
    "            print('error')\n",
    "            max_error +=1\n",
    "        if(error_data_number != 0):\n",
    "            for j in range(len(data[i])):\n",
    "                if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                    if(data[i][j,4] == 'clickout item'):\n",
    "                        click_check = True\n",
    "        if(click_check == False):\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "            #print(data[i])\n",
    "            #print('datas')\n",
    "        if(max_error > 10):\n",
    "            break\n",
    "            \n",
    "            \n",
    "def check_inference(data):\n",
    "    datalength = len(data)\n",
    "    maxlength = 0\n",
    "    correct_data = 0\n",
    "    new_data = []\n",
    "    for i in range(datalength):\n",
    "        if(data[i][-1,5] in data[i][-1,10].split('|')):\n",
    "            new_data.append(data[i])\n",
    "            correct_data += 1\n",
    "        else:\n",
    "            pass\n",
    "            #print(i)\n",
    "            #print('error')\n",
    "            #maxlength +=1\n",
    "    print(correct_data)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 트레이닝 데이터를 세션 단위로 나눔\n",
    "def make_data(data): \n",
    "    data_split = []\n",
    "    temp_data = []\n",
    "    train_size = len(data)\n",
    "    action_index = 1\n",
    "    for i in range(train_size):\n",
    "        if(i==0):\n",
    "            temp_data.append(data[i])\n",
    "        if(i>0):\n",
    "            if(data[i,action_index] == data[i-1,action_index]):\n",
    "                temp_data.append(data[i])\n",
    "            else:\n",
    "                data_split.append(np.array(temp_data))\n",
    "                temp_data = []\n",
    "                temp_data.append(data[i])\n",
    "    return data_split\n",
    "\n",
    "train_data_split = make_data(df_train.values) #session 별로 데이터 나눔\n",
    "\n",
    "#트레이닝 데이터에서 click out이 존재하는 데이터만 뽑아냄\n",
    "#트레이닝 데이터에서 끝부분이 click out이 되도록 뒷부분을 자름\n",
    "def data_preprocess(data): \n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        check_data = False\n",
    "        final_click = 0\n",
    "        for j in range(len(data[i])):\n",
    "            if(data[i][j,4] =='clickout item'):\n",
    "                final_click = j\n",
    "                check_data = True\n",
    "        if(final_click < len(data[i])-1 and check_data):\n",
    "            new_data.append(np.delete(data[i],np.s_[final_click+1:],0))\n",
    "        elif(check_data):\n",
    "            new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data_preprocess = data_preprocess(train_data_split) # session 끝에 무조건 clickout이 존재\n",
    "train_data_preprocess = check_inference(train_data_preprocess) # impresssion 안에 무조건 답이 존재 하는것!!\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "#reference embedding 보조 함수\n",
    "items = df_meta['item_id'].values\n",
    "properties = df_meta['properties'].values\n",
    "all_items_test = []\n",
    "for i in range(len(properties)):\n",
    "    p_data = properties[i].split('|')\n",
    "    all_items_test += p_data\n",
    "set_items = set(all_items_test)\n",
    "\n",
    "list_items = list(set_items)\n",
    "list_items.sort()\n",
    "def embedding_item_diction():\n",
    "    all_items = {}\n",
    "    for i in range(len(properties)):\n",
    "        all_items[items[i]] = properties[i].split('|')\n",
    "    embedding_items = {}\n",
    "    #print(len(list_items))\n",
    "    for keys in all_items.keys():\n",
    "        item_embeddings = [0]*len(list_items)\n",
    "        for i in range(len(list_items)):\n",
    "            if list_items[i] in all_items[keys]:\n",
    "                item_embeddings[i] = 1\n",
    "        embedding_items[keys] = item_embeddings\n",
    "    return embedding_items\n",
    "reference_diction = embedding_item_diction()\n",
    "\n",
    "#filter embedding 보조함수\n",
    "all_filters = []\n",
    "for i in range(len(filter_list)):\n",
    "    if i>0:\n",
    "        all_filters += filter_list[i].split('|')\n",
    "all_filter_set = set(all_filters)\n",
    "all_filter_list = list(all_filter_set)\n",
    "all_filter_list.sort()\n",
    "def filter_embedding(data, version):\n",
    "    embedding_data = [0]*202\n",
    "    if version == 1:\n",
    "        datalist = data.split('|')\n",
    "        for filters in datalist:\n",
    "            embedding_data[all_filter_list.index(filters)] = 1\n",
    "        return embedding_data\n",
    "    \n",
    "def one_hot_encoding(number, length):\n",
    "    return [int(i==number) for i in range(length)]\n",
    "\n",
    "def sum_list(list1, list2):\n",
    "    length = len(list1)\n",
    "    final_list = []\n",
    "    for i in range(length):\n",
    "        final_list.append(list1[i] +list2[i])\n",
    "    return final_list\n",
    "\n",
    "# 전체 action embedding\n",
    "def embedding(data, action_index):\n",
    "    if action_index == 0: #user_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 1: # session_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 2: # timestep dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 3: #step dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 4: #action_type dim 10\n",
    "        #print(one_hot_encoding(action_list.index(data), 10))\n",
    "        return one_hot_encoding(action_list.index(data), 10)\n",
    "    \n",
    "    if action_index == 5: #reference dim 157\n",
    "        if RepresentsInt(data):\n",
    "            return reference_diction[int(data)]\n",
    "        else:\n",
    "            return [0]*157\n",
    "        \n",
    "    if action_index == 6: #platform dim 55\n",
    "        return one_hot_encoding(platform_list.index(data), 55)\n",
    "    \n",
    "    if action_index == 7: #city dim 0\n",
    "        #return city_list.index(data)\n",
    "        return []\n",
    "    \n",
    "    if action_index == 8: #device dim 3\n",
    "        return one_hot_encoding(device_list.index(data), 3)\n",
    "    \n",
    "    if action_index == 9: #current_filters dim 202\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*202\n",
    "        else:\n",
    "            return filter_embedding(data, 1)\n",
    "        \n",
    "    if action_index == 10: #impressions dim 157\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*157\n",
    "        else:\n",
    "            impression_embedding = [0]*157\n",
    "            impression_list = data.split('|')\n",
    "            for impression in impression_list:\n",
    "                try:\n",
    "                    impression_embedding =sum_list(impression_embedding, reference_diction[int(impression)])\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "                    #print(int(impression))\n",
    "            '''\n",
    "            if(len(impression_embedding) < 3925):\n",
    "                impression_embedding += [0]*(3925-len(impression_embedding))   \n",
    "            '''\n",
    "            return impression_embedding\n",
    "            \n",
    "        \n",
    "    if action_index == 11: #prices dim 25\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*25\n",
    "        else:\n",
    "            price_embedding = []\n",
    "            for prices in data.split('|'):\n",
    "                price_embedding.append(int(prices))\n",
    "            if(len(price_embedding) < 25):\n",
    "                price_embedding += [0]*(25-len(price_embedding))   \n",
    "            return price_embedding\n",
    "        \n",
    "def check_last_click(data):\n",
    "    data_number = 0\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][-1,4] == 'clickout item'):\n",
    "            data_number +=1\n",
    "    return data_number\n",
    "\n",
    "action_type_length = 10\n",
    "reference_length = 157\n",
    "platform_length = 55\n",
    "device_length = 3\n",
    "current_filters_length = 202\n",
    "impresssions_length = 3925\n",
    "prices_length = 25\n",
    "def addVector(vector1, vector2, coef, action_num):\n",
    "    new_action_type = []\n",
    "    new_reference = []\n",
    "    new_platform = []\n",
    "    new_device = []\n",
    "    new_filters = []\n",
    "    new_impressions = []\n",
    "    new_prices = [] \n",
    "    #print(len(vector1))\n",
    "    #print(len(vector2))\n",
    "    for i in range(10):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "    \n",
    "    for i in range(10, 167):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    for i in range(167, 222):\n",
    "        new_action_type.append(vector1[i])\n",
    "    \n",
    "    for i in range(222, 225):\n",
    "        new_action_type.append(vector1[i])\n",
    "        \n",
    "    for i in range(225, 427):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    return new_action_type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeEmbeddingVector(data):\n",
    "    item_vector = [] # item data embedding\n",
    "    num_action = len(data)\n",
    "    correct_index = []\n",
    "    session_vector = []\n",
    "    for i in range(num_action): #거꾸로\n",
    "        temp_vector = []\n",
    "        for j in range(10):\n",
    "            temp_vector += embedding(data[i,j], j) #10+157+55+3+202 = 427\n",
    "        if(i == num_action-1):\n",
    "            temp_vector[10:167] = [0]*157 # 마지막 action의 reference를 지움\n",
    "        session_vector.append(temp_vector)\n",
    "    # 10+314+202+55+3 = 584\n",
    "    '''\n",
    "    for j in range(12):\n",
    "        timestep_vector += embedding(data[i,j], j) \n",
    "\n",
    "    if(i == num_action-1):\n",
    "        for idx in range(10, 167):\n",
    "            timestep_vector[idx] = 0 # reference 부분을 0으로 만듦\n",
    "    session_vector = addVector(session_vector, timestep_vector, time_delay_coef, num_action-1-i) #session vector 생성\n",
    "    '''\n",
    "    #item_vector.append(reference_diction[int(data[-1,5])])\n",
    "    temp_impressions = data[-1, 10].split('|')\n",
    "    for i in range(len(temp_impressions)):\n",
    "        if(data[-1,5] == temp_impressions[i]):\n",
    "            correct_index.append(i)\n",
    "        item_vector.append(reference_diction[int(temp_impressions[i])])\n",
    "    return session_vector, item_vector, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevec, itemvec, crindex = MakeEmbeddingVector(train_data_preprocess[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.743267391538431\n"
     ]
    }
   ],
   "source": [
    "datalength = len(train_data_preprocess)\n",
    "average_step =0\n",
    "steps = []\n",
    "for i in range(datalength):\n",
    "    average_step += train_data_preprocess[i][-1,3]\n",
    "    steps.append(train_data_preprocess[i][-1,3])\n",
    "print(average_step/datalength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 35, 7, 55, 3, 1, 1, 7, 1, 34]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMedian(a):\n",
    "    a_len = len(a)                # 배열 요소들의 전체 개수 구하기\n",
    "    if (a_len == 0): return None  # 빈 배열은 에러 반환\n",
    "    a_center = int(a_len / 2)     # 요소 개수의 절반값 구하기\n",
    "\n",
    "    if (a_len % 2 == 1):   # 요소 개수가 홀수면\n",
    "        return a[a_center]   # 홀수 개수인 배열에서는 중간 요소를 그대로 반환\n",
    "    else:\n",
    "        return (a[a_center - 1] + a[a_center]) / 2.0 # 짝수 개 요소는, 중간 두 수의 평균 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMedian(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(session, length = 10):\n",
    "    if(len(session) < length):\n",
    "        for i in range(length-len(session)):\n",
    "            session.append([0]*427)\n",
    "    else:\n",
    "        session = session[:length]\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sevec = padding(sevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hidden_size = 157\n",
    "sequence_length = 10\n",
    "embedding_length = 427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session_placeholder = tf.placeholder(tf.float32, shape=(None, sequence_length, embedding_length))\n",
    "item_placeholder = tf.placeholder(tf.float32, shape=(None, hidden_size))\n",
    "#item_length = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 157) dtype=float32>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnNetwork():\n",
    "\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    output, state = tf.nn.dynamic_rnn(cell, session_placeholder, dtype = tf.float32)\n",
    "    return output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseNet(rnn_output):\n",
    "    final_output = []\n",
    "    for items in item_placeholder:\n",
    "        dense_input = tf.concat(rnn_output,items)\n",
    "        dense_layer1 = tf.layers.dense(dense_input, 200, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dense_layer2 = tf.layers.dense(dense_layer1, 50, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dense_layer3 = tf.layers.dense(dense_layer2, 1, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        final_output.append(dense_layer3)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-fdf6bfe7ea71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-11a4813d424a>\u001b[0m in \u001b[0;36mdenseNet\u001b[0;34m(rnn_output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdense_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdense_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       raise TypeError(\n\u001b[0;32m--> 442\u001b[0;31m           \u001b[0;34m\"Tensor objects are only iterable when eager execution is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[1;32m    444\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn."
     ]
    }
   ],
   "source": [
    "score = denseNet(rnnNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(tf.math.log(tf.sigmoid(scores[train_index[0]] - scores)), 0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
